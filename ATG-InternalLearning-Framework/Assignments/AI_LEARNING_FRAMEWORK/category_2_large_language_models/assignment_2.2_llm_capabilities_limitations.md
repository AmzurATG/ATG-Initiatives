# Lesson Plan Assignment: LLM Capabilities & Limitations

---

## Assignment Information

**Category:** Category 2: Large Language Models (LLMs)  
**Subcategory:** 2.2 LLM Capabilities & Limitations  
**Assigned To:** [Team Member 1 & Team Member 2]  
**Presentation Date:** [Date]  
**Duration:** 30-45 minutes

---

## Learning Objectives

By the end of this session, team members should be able to:
1. Identify what tasks LLMs excel at and where they fall short
2. Recognize and handle hallucinations in LLM outputs
3. Manage token limits and context window constraints effectively

---

## Topics to Cover

- **LLM Capabilities:** Text generation, summarization, translation, reasoning, code generation
- **LLM Limitations:** Hallucinations, knowledge cutoff, mathematical reasoning, factual accuracy
- **Reasoning Abilities:** What counts as "reasoning" in LLMs, chain-of-thought capabilities
- **Context Management:** Token counting, context window strategies, memory limitations

---

## Assignment Deliverables

### 1. Presentation (Required)
- Create slides showing real examples of LLM capabilities and failures
- Include case studies of hallucinations and how to detect them
- Duration: 20-30 minutes

### 2. Code Demo/Example (Required)
- Demonstrate token counting and context window management
- Show examples of hallucinations and verification techniques
- Create a tool to track token usage in conversations

### 3. Resources & References (Required)
- Curate 3-5 resources on LLM limitations and best practices
- Create a "Do's and Don'ts" guide for LLM usage
- Share techniques for hallucination detection and mitigation

### 4. Q&A Session (Required)
- Prepare for 10-15 minutes of questions
- Discussion questions:
  - "How do we verify LLM outputs in production systems?"
  - "When should we NOT use LLMs for a task?"

---

## Preparation Guidelines

- **Research:** Test multiple LLMs with edge cases to find limitations
- **Practice:** Document examples of hallucinations and failures
- **Collaborate:** One focuses on capabilities, other on limitations and mitigations
- **Engage:** Show live demos of LLM strengths and weaknesses

---

## Success Criteria

- [ ] Team can identify appropriate use cases for LLMs
- [ ] Clear understanding of hallucination risks and mitigation strategies
- [ ] Working examples demonstrate token/context management
- [ ] Team knows when NOT to use LLMs
- [ ] Practical guide provided for verification and validation

---

## Support & Resources

- Reference: AI_Learning_Framework.md - Category 2.2
- OpenAI, Anthropic documentation on model limitations
- Research papers on LLM hallucinations
- Share draft materials 2 days before presentation

---

## Notes

- Use real-world examples from projects or industry
- Emphasize verification and validation in production
- Show both impressive capabilities AND critical failures
- This is crucial for responsible LLM deployment
