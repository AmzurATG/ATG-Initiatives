# Lesson Plan Assignment: Fine-tuning & Customization

---

## Assignment Information

**Category:** Category 3: Generative AI  
**Subcategory:** 3.3 Fine-tuning & Customization  
**Assigned To:** [Team Member 1 & Team Member 2]  
**Presentation Date:** [Date]  
**Duration:** 30-45 minutes

---

## Learning Objectives

By the end of this session, team members should be able to:
1. Understand different fine-tuning approaches and when to use them
2. Apply parameter-efficient methods like LoRA and QLoRA
3. Evaluate whether fine-tuning or alternatives (RAG, prompting) are appropriate

---

## Topics to Cover

- **Fine-tuning Approaches:** Full fine-tuning, instruction tuning, RLHF overview
- **Parameter-Efficient Methods:** LoRA (Low-Rank Adaptation), QLoRA, adapters
- **Domain Adaptation:** Training on domain-specific data, data preparation
- **Evaluation:** How to evaluate fine-tuned models, benchmarking, quality metrics

---

## Assignment Deliverables

### 1. Presentation (Required)
- Create slides explaining fine-tuning concepts and approaches
- Include decision framework: when to fine-tune vs RAG vs prompting
- Duration: 20-30 minutes

### 2. Code Demo/Example (Required)
- Demonstrate fine-tuning a small model with LoRA/QLoRA
- Show before/after comparison of model performance
- Build evaluation script to measure improvement

### 3. Resources & References (Required)
- Curate 3-5 resources on fine-tuning techniques and best practices
- Create cost comparison: fine-tuning vs RAG vs prompting
- Share dataset preparation guidelines and code templates

### 4. Q&A Session (Required)
- Prepare for 10-15 minutes of questions
- Discussion questions:
  - "For our use cases, should we fine-tune or use RAG?"
  - "What's the ROI of investing in fine-tuning?"

---

## Preparation Guidelines

- **Research:** Study Hugging Face PEFT library and fine-tuning guides
- **Practice:** Fine-tune a model on a sample dataset (customer support, code, etc.)
- **Collaborate:** One covers theory/approaches, other demonstrates implementation
- **Engage:** Show concrete metrics improvement from fine-tuning

---

## Success Criteria

- [ ] Team understands when and why to fine-tune models
- [ ] Parameter-efficient methods (LoRA/QLoRA) clearly explained
- [ ] Working fine-tuning example with measurable improvements
- [ ] Decision framework created for choosing approach
- [ ] Cost and effort estimates provided for planning

---

## Support & Resources

- Reference: AI_Learning_Framework.md - Category 3.3
- Hugging Face PEFT library documentation
- OpenAI fine-tuning guides
- Share draft materials 2 days before presentation

---

## Notes

- Emphasize practical decision-making criteria
- Compare costs: fine-tuning vs RAG vs better prompts
- Show real improvement metrics, not just anecdotes
- Connect back to Category 4 (RAG) for comparison
